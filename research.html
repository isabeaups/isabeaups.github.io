<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title data-en="Research - Isabeau Prémont-Schwarz" data-fr="Recherche - Isabeau Prémont-Schwarz">Research - Isabeau Prémont-Schwarz</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Language Toggle -->
    <div class="language-toggle">
        <button id="lang-en" class="lang-btn active" onclick="switchLanguage('en')">EN</button>
        <button id="lang-fr" class="lang-btn" onclick="switchLanguage('fr')">FR</button>
    </div>

    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-title">
                <span data-en="Isabeau Prémont-Schwarz" data-fr="Isabeau Prémont-Schwarz">Isabeau Prémont-Schwarz</span>
            </a>
            <button class="mobile-menu-toggle" onclick="toggleMobileMenu()">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu" id="navMenu">
                <li><a href="index.html" data-en="Home" data-fr="Accueil">Home</a></li>
                <li><a href="research.html" class="active" data-en="Research" data-fr="Recherche">Research</a></li>
                <li><a href="team.html" data-en="Team" data-fr="Équipe">Team</a></li>
                <li><a href="teaching.html" data-en="Teaching" data-fr="Enseignement">Teaching</a></li>
                <li><a href="prospective.html" data-en="Prospective Students" data-fr="Futur(e)s étudiant(e)s">Prospective Students</a></li>
            </ul>
        </div>
    </nav>

    <!-- Main Content -->
    <main class="main-content">
        <div class="container">
            <h1 data-en="Research" data-fr="Recherche">Research</h1>

            <!-- Research Interests -->
            <section class="content-section">
                <h2 data-en="Research Interests" data-fr="Intérêts de recherche">Research Interests</h2>
                <p data-en="My research is in machine learning, including reinforcement learning. I am very interested in representation learning and brain-inspired algorithms, abstractions, and generalization."
                       data-fr="Mes recherches portent sur l'apprentissage automatique, notamment l'apprentissage par renforcement. Je m'intéresse beaucoup à l'apprentissage de représentations et aux algorithmes inspirés du cerveau, aux abstractions et à la généralisation.">
                        My research is in machine learning, including reinforcement learning. I am very interested in representation learning and brain-inspired algorithms, abstractions, and generalization.
                    </p>
                <ul class="simple-list">
                    <li data-en="Reinforcement Learning" data-fr="Apprentissage par renforcement">Reinforcement Learning</li>
                    <li data-en="Combinatorial Generalization" data-fr="Généralisation combinatoire">Combinatorial Generalization</li>
                    <li data-en="Brain-inspired AI" data-fr="IA inspirée du cerveau">Brain-inspired AI</li>
                    <li data-en="Unsupervised Abstractions" data-fr="Abstractions non-supervisées">Unsupervised Abstractions</li>
                </ul>
            </section>

            <!-- Current Projects -->
            <section class="content-section">
                <h2 data-en="Current Research Projects" data-fr="Projets de recherche actuels">Current Research Projects</h2>
                
                <div class="research-project">
                    <h3 data-en="Combinatorial Generalization with Tensorial Representations" data-fr="Généralisation combinatoire avec représentations tensorielles">Combinatorial Generalization with Tensorial Representations</h3>

<section
  id="combinatorial-generalization"
    markdown=True
  data-en='# Combinatorial Generalization in Neural Networks

Current neural networks fail at combinatorial generalization -- recombining known concepts in novel ways -- because they destroy compositional structure at every layer. Previous approaches focused on disentangled representations, but this addresses only half the problem.

My hypothesis is that combinatorial generalization requires both structured representations and operations that preserve this structure. This research develops inductive biases in representations and operations for neural networks which favour combinatorial generalization and then uses these principles to build neural architectures for purposes such as vision, and sequences and demonstrate applications in language modeling -- including for under-resourced Indigenous languages -- forestry robotics.

We further aim to prove theoretical combinatorial generalization bounds.'
  data-fr='# Généralisation combinatoire dans les réseaux de neurones

Les réseaux de neurones actuels échouent à la généralisation combinatoire — recombiner des concepts connus de manière nouvelle — parce qu’ils détruisent la structure compositionnelle à chaque couche. Les approches précédentes se sont concentrées sur des représentations « désentremêlées », mais cela ne résout que la moitié du problème.

Mon hypothèse est que la généralisation combinatoire requiert à la fois des représentations structurées et des opérations qui préservent cette structure. Cette recherche développe des biais inductifs dans les représentations et les opérations des réseaux de neurones qui favorisent la généralisation combinatoire, puis utilise ces principes pour concevoir des architectures pour la vision, les séquences, et pour démontrer des applications en modélisation de langue — y compris pour des langues autochtones sous-dotées — ainsi qu’en robotique forestière.

Nous visons également à établir des bornes théoriques de généralisation combinatoire.'
>
</section>

                </div>

                <div class="research-project">
                    <h3 data-en="RL-Map" data-fr="Apprentissage par renforcement cartographique">RL-Map</h3>
                    <p data-en="[Description of the project, including objectives, methodology, and expected outcomes. Include funding sources and collaborators.]"
                       data-fr="[Description du projet, y compris les objectifs, la méthodologie et les résultats attendus. Inclure les sources de financement et les collaborateurs.]">
                        [Description of the project, including objectives, methodology, and expected outcomes. Include funding sources and collaborators.]
                    </p>
                </div>

                <div class="research-project">
                    <h3 data-en="Deep Local Unsupervised Learning" data-fr="Apprentissage local non-supervisé profond">Deep Local Unsupervised Learning</h3>
                    
<!--    <p><strong>Contemporary deep learning is built on three foundational pillars:</strong></p>
    <ol>
        <li><strong>Artificial neural networks with at least one hidden layer</strong> (that is, at least one layer of neurons between the input and the output),</li>
        <li><strong>The formulation of an objective function</strong> that takes as input data and learnable parameters (the “synapses”) of the neural network and defines the AI’s performance by outputting a real number; the smaller this number, the better the AI performs, and</li>
        <li><strong>The adjustment of the network’s “synapses” using the gradient backpropagation algorithm</strong>, which simply means that learning occurs by attempting to minimize the objective function by descending its gradient with respect to the synapses.</li>
    </ol>

    <p>However, we know that the human brain does not learn by trying to minimize a global objective function. Instead, it learns through local learning rules at the level of individual neurons, such as Hebbian learning. Despite this, the brain is capable of developing multi-layered deep representations.</p>

    <p>We also know that local learning, such as Hebbian learning—which does not depend on a global objective function—is more robust, generalizes better beyond the training data (a major shortcoming of current deep learning), and is more resistant to catastrophic forgetting (the inability in current deep learning to learn multiple tasks sequentially without having to learn them all in parallel).</p>

    <p>That said, implementing Hebbian learning (or other unsupervised local learning) in artificial neural networks remains very challenging and performs poorly in deep learning contexts. This project addresses this problem and aims to develop unsupervised local learning rules that can still achieve effective learning in multi-layer neural networks (that is, in deep learning).</p>
                    
                    
      -->              
<!--                    <p data-en="Contemporary deep learning is built on three foundational pillars:
Artificial neural networks with at least one hidden layer (that is, at least one layer of neurons between the input and the output),
The formulation of an objective function that takes as input data and learnable parameters (the “synapses”) of the neural network and defines the AI’s performance by outputting a real number; the smaller this number, the better the AI performs, and
The adjustment of the network’s “synapses” using the gradient backpropagation algorithm, which simply means that learning occurs by attempting to minimize the objective function by descending its gradient with respect to the synapses.

However, we know that the human brain does not learn by trying to minimize a global objective function. Instead, it learns through local learning rules at the level of individual neurons, such as Hebbian learning. Despite this, the brain is capable of developing multi-layered deep representations. We also know that local learning, such as Hebbian learning—which does not depend on a global objective function—is more robust, generalizes better beyond the training data (a major shortcoming of current deep learning), and is more resistant to catastrophic forgetting (the inability in current deep learning to learn multiple tasks sequentially without having to learn them all in parallel).
That said, implementing Hebbian learning (or other unsupervised local learning) in artificial neural networks remains very challenging and performs poorly in deep learning contexts. This project addresses this problem and aims to develop unsupervised local learning rules that can still achieve effective learning in multi-layer neural networks."
                       data-fr="L'apprentissage profond contemporain se fonde sur trois piliers porteurs: 1) les réseaux de neurones artificiels avec au moins une couche cachée (c'est-à-dire au moins une couche de neurones entre l'entrée et la sortie), 2) la formulation d'une fonction objective qui prend en entrée des données et des paramètres à apprendre (les « synapses ») du réseau de neurones et qui définie la performance de l'IA en donnant un réel en sortie; plus ce réel est petit mieux est la performance de l'IA, et 3) la modification des « synapses » du réseau de neurones en utilisant l'algorithme de rétropropagation du gradient (backpropagation), qui veut dire simplement que l'apprentissage se fait en essayer de minimizer la fonction objective en descendant le gradient de cette fonction par rapport aux « synapes ». 

Nous savons pourtant que le cerveau humain n'apprend pas, en essayant de minimiser une fonction objective globale, mais plutôt apprend par des règles d'apprentissage locales aux neurones telle l'apprentissage Hebbien. Malgré cela, le cerveau est capables de développer des représentation en plusieurs couches profondes. Nous savons aussi qu'un apprentissage local tel l'apprentissage Hebbien qui n'est pas dépendant d'une fonction objective globale est plus robuste, sait mieux généraliser hors des données d'apprentissage (une grande lacune de l'apprentissage profond actuel), et résiste mieux à l'oublie catastrophique (le fait en apprentissage profond actuel de ne pas être capable d'apprendre plusieurs tâches en série et de devoir absolument les apprendre en parallèle). 

Ceci dit, l'apprentissage Hebbien (ou autre apprentissage local non-supervisé) avec des réseaux neurones artificiels est encore très difficile et peu performant en apprentissage profond. Ce projet s'attaque à ce problème et vise à développer un apprentissage avec des règles locales non-supervisées qui arrivent tout de même à bien apprendre dans un contexte de réseaux neurones multi-couche (c'est-à-dire d'apprentissage profond). ">
                        Contemporary deep learning is built on three foundational pillars:
1) Artificial neural networks with at least one hidden layer (that is, at least one layer of neurons between the input and the output),
2) The formulation of an objective function that takes as input data and learnable parameters (the “synapses”) of the neural network and defines the AI’s performance by outputting a real number; the smaller this number, the better the AI performs, and
3) The adjustment of the network’s “synapses” using the gradient backpropagation algorithm, which simply means that learning occurs by attempting to minimize the objective function by descending its gradient with respect to the synapses.

However, we know that the human brain does not learn by trying to minimize a global objective function. Instead, it learns through local learning rules at the level of individual neurons, such as Hebbian learning. Despite this, the brain is capable of developing multi-layered deep representations. We also know that local learning, such as Hebbian learning—which does not depend on a global objective function—is more robust, generalizes better beyond the training data (a major shortcoming of current deep learning), and is more resistant to catastrophic forgetting (the inability in current deep learning to learn multiple tasks sequentially without having to learn them all in parallel).
That said, implementing Hebbian learning (or other unsupervised local learning) in artificial neural networks remains very challenging and performs poorly in deep learning contexts. This project addresses this problem and aims to develop unsupervised local learning rules that can still achieve effective learning in multi-layer neural networks.
                    </p>-->
                

<p
  data-en="Contemporary deep learning is built on three foundational pillars:"
  data-fr="L'apprentissage profond contemporain se fonde sur trois piliers porteurs :">
    Contemporary deep learning is built on three foundational pillars:
</p>

<p
  data-en="1) Artificial neural networks with at least one hidden layer (that is, at least one layer of neurons between the input and the output),"
  data-fr="1) Les réseaux de neurones artificiels avec au moins une couche cachée (c'est-à-dire au moins une couche de neurones entre l'entrée et la sortie),">
1) Artificial neural networks with at least one hidden layer (that is, at least one layer of neurons between the input and the output),</p>

<p
  data-en="2) The formulation of an objective function that takes as input data and learnable parameters (the “synapses”) and outputs a real number measuring performance; the smaller this number, the better the model,"
  data-fr="2) La formulation d'une fonction objective qui prend en entrée des données et des paramètres à apprendre (les « synapses ») et retourne un réel mesurant la performance ; plus il est petit, meilleure est l'IA,">
2) The formulation of an objective function that takes as input data and learnable parameters (the “synapses”) and outputs a real number measuring performance; the smaller this number, the better the model,</p>

<p
  data-en="3) The adjustment of the network’s “synapses” using gradient backpropagation, which means minimizing the objective function via gradient descent."
  data-fr="3) La modification des « synapses » du réseau en utilisant la rétropropagation du gradient, c’est-à-dire en minimisant la fonction objective via descente de gradient.">
3) The adjustment of the network’s “synapses” using gradient backpropagation, which means minimizing the objective function via gradient descent.</p>

<p
  data-en="However, we know that the human brain does not learn by minimizing a global objective function, but instead through local learning rules such as Hebbian learning."
  data-fr="Nous savons pourtant que le cerveau humain n'apprend pas en minimisant une fonction objective globale, mais via des règles locales telles que l'apprentissage hebbien.">
However, we know that the human brain does not learn by minimizing a global objective function, but instead through local learning rules such as Hebbian learning.</p>

<p
  data-en="Local learning rules are more robust, generalize better beyond training data, and are more resistant to catastrophic forgetting."
  data-fr="Les règles d'apprentissage locales sont plus robustes, généralisent mieux hors des données d'entraînement et résistent mieux à l'oubli catastrophique.">
Local learning rules are more robust, generalize better beyond training data, and are more resistant to catastrophic forgetting.</p>

<p
  data-en="However, implementing Hebbian or other unsupervised local learning rules in deep artificial neural networks remains challenging, and this project aims to address that challenge."
  data-fr="Cependant, implémenter l’apprentissage hebbien ou d’autres formes d’apprentissage local non supervisé dans des réseaux de neurones profonds reste très difficile, et ce projet vise à relever ce défi.">
However, implementing Hebbian or other unsupervised local learning rules in deep artificial neural networks remains challenging, and this project aims to address that challenge.</p>


                </div>
            </section>

            <!-- Publications Section - Integrated from Google Scholar -->
            <section class="content-section">
                <h2 data-en="Publications" data-fr="Publications">Publications</h2>
<!--                <p data-en="For a complete list of publications, please visit my Google Scholar profile."
                   data-fr="Pour une liste complète des publications, veuillez consulter mon profil Google Scholar.">
                    For a complete list of publications, please visit my Google Scholar profile.
                </p>-->
                
                <div class="publication-links">
                    <a href="https://www.semanticscholar.org/author/Isabeau-Pr'emont-Schwarz/1403769483" target="_blank" class="btn-primary">
                        <span data-en="View Semantic Scholar Profile" data-fr="Voir le profil Semantic Scholar">View Semantic Scholar Profile</span>
                    </a>
                </div>

                <!-- Optional: Embedded Google Scholar Widget -->
                <!-- You can replace the iframe src with your actual Google Scholar profile -->
                <!--
                <div class="scholar-embed">
                    <iframe src="https://scholar.google.com/citations?user=YOUR_SCHOLAR_ID&hl=en" 
                            width="100%" 
                            height="600" 
                            frameborder="0"></iframe>
                </div>
                -->

                <!-- Alternative: Selected Publications (if you want to manually highlight a few) -->
<!--                <h3 data-en="Selected Recent Publications" data-fr="Publications récentes sélectionnées">Selected Recent Publications</h3>
                <ul class="simple-list">
                    <li data-en="[Leave this section for your most important recent papers if desired, otherwise remove]"
                        data-fr="[Laissez cette section pour vos articles récents les plus importants si désiré, sinon supprimez]">
                        [Leave this section for your most important recent papers if desired, otherwise remove]
                    </li>
                </ul>-->
            </section>

            <!-- Collaborators -->
<!--            <section class="content-section">
                <h2 data-en="Collaborators" data-fr="Collaborateurs">Collaborators</h2>
                <p data-en="I am fortunate to collaborate with researchers from various institutions:"
                   data-fr="J'ai la chance de collaborer avec des chercheurs de diverses institutions :">
                    I am fortunate to collaborate with researchers from various institutions:
                </p>
                <ul class="simple-list">
                    <li><a href="#">[Collaborator Name]</a>, [Institution]</li>
                    <li><a href="#">[Collaborator Name]</a>, [Institution]</li>
                    <li><a href="#">[Collaborator Name]</a>, [Institution]</li>
                </ul>
            </section>-->
        </div>
    </main>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2025 <span data-en="Isabeau Prémont-Schwarz" data-fr="Isabeau Prémont-Schwarz">Isabeau Prémont-Schwarz</span></p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>