<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title data-en="Research - Isabeau Prémont-Schwarz" data-fr="Recherche - Isabeau Prémont-Schwarz">Research - Isabeau Prémont-Schwarz</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Language Toggle -->
    <div class="language-toggle">
        <button id="lang-en" class="lang-btn active" onclick="switchLanguage('en')">EN</button>
        <button id="lang-fr" class="lang-btn" onclick="switchLanguage('fr')">FR</button>
    </div>

    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-title">
                <span data-en="Isabeau Prémont-Schwarz" data-fr="Isabeau Prémont-Schwarz">Isabeau Prémont-Schwarz</span>
            </a>
            <button class="mobile-menu-toggle" onclick="toggleMobileMenu()">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu" id="navMenu">
                <li><a href="index.html" data-en="Home" data-fr="Accueil">Home</a></li>
                <li><a href="research.html" class="active" data-en="Research" data-fr="Recherche">Research</a></li>
                <li><a href="team.html" data-en="Team" data-fr="Équipe">Team</a></li>
                <li><a href="teaching.html" data-en="Teaching" data-fr="Enseignement">Teaching</a></li>
                <li><a href="prospective.html" data-en="Prospective Students" data-fr="Futur(e)s étudiant(e)s">Prospective Students</a></li>
            </ul>
        </div>
    </nav>

    <!-- Main Content -->
    <main class="main-content">
        <div class="container">
            <h1 data-en="Research" data-fr="Recherche">Research</h1>

            <!-- Research Interests -->
            <section class="content-section">
                <h2 data-en="Research Interests" data-fr="Intérêts de recherche">Research Interests</h2>
                <p data-en="My research is in machine learning, including reinforcement learning. I am very interested in representation learning and brain-inspired algorithms, abstractions, and generalization."
                       data-fr="Mes recherches portent sur l'apprentissage automatique, notamment l'apprentissage par renforcement. Je m'intéresse beaucoup à l'apprentissage de représentations et aux algorithmes inspirés du cerveau, aux abstractions et à la généralisation.">
                        My research is in machine learning, including reinforcement learning. I am very interested in representation learning and brain-inspired algorithms, abstractions, and generalization.
                    </p>
                <ul class="simple-list">
                    <li data-en="Reinforcement Learning" data-fr="Apprentissage par renforcement">Reinforcement Learning</li>
                    <li data-en="Combinatorial Generalization" data-fr="Généralisation combinatoire">Combinatorial Generalization</li>
                    <li data-en="Brain-inspired AI" data-fr="IA inspirée du cerveau">Brain-inspired AI</li>
                    <li data-en="Unsupervised Abstractions" data-fr="Abstractions non-supervisées">Unsupervised Abstractions</li>
                </ul>
            </section>

            <!-- Current Projects -->
            <section class="content-section">
                <h2 data-en="Current Research Projects" data-fr="Projets de recherche actuels">Current Research Projects</h2>
                
                <div class="research-project">
                    <h3 data-en="Combinatorial Generalization with Tensorial Representations" data-fr="[Titre du projet 1]">Combinatorial Generalization with Tensorial Representations</h3>
                    <p data-en="[Description of the project, including objectives, methodology, and expected outcomes. Include funding sources and collaborators.]"
                       data-fr="[Description du projet, y compris les objectifs, la méthodologie et les résultats attendus. Inclure les sources de financement et les collaborateurs.]">
                        [Description of the project, including objectives, methodology, and expected outcomes. Include funding sources and collaborators.]
                    </p>
                </div>

                <div class="research-project">
                    <h3 data-en="RL-Map" data-fr="Apprentissage par renforcement cartographique">RL-Map</h3>
                    <p data-en="[Description of the project, including objectives, methodology, and expected outcomes. Include funding sources and collaborators.]"
                       data-fr="[Description du projet, y compris les objectifs, la méthodologie et les résultats attendus. Inclure les sources de financement et les collaborateurs.]">
                        [Description of the project, including objectives, methodology, and expected outcomes. Include funding sources and collaborators.]
                    </p>
                </div>

                <div class="research-project">
                    <h3 data-en="Deep Local Unsupervised Learning" data-fr="Apprentissage local non-supervisé profond">Deep Local Unsupervised Learning</h3>
                    
<!--    <p><strong>Contemporary deep learning is built on three foundational pillars:</strong></p>
    <ol>
        <li><strong>Artificial neural networks with at least one hidden layer</strong> (that is, at least one layer of neurons between the input and the output),</li>
        <li><strong>The formulation of an objective function</strong> that takes as input data and learnable parameters (the “synapses”) of the neural network and defines the AI’s performance by outputting a real number; the smaller this number, the better the AI performs, and</li>
        <li><strong>The adjustment of the network’s “synapses” using the gradient backpropagation algorithm</strong>, which simply means that learning occurs by attempting to minimize the objective function by descending its gradient with respect to the synapses.</li>
    </ol>

    <p>However, we know that the human brain does not learn by trying to minimize a global objective function. Instead, it learns through local learning rules at the level of individual neurons, such as Hebbian learning. Despite this, the brain is capable of developing multi-layered deep representations.</p>

    <p>We also know that local learning, such as Hebbian learning—which does not depend on a global objective function—is more robust, generalizes better beyond the training data (a major shortcoming of current deep learning), and is more resistant to catastrophic forgetting (the inability in current deep learning to learn multiple tasks sequentially without having to learn them all in parallel).</p>

    <p>That said, implementing Hebbian learning (or other unsupervised local learning) in artificial neural networks remains very challenging and performs poorly in deep learning contexts. This project addresses this problem and aims to develop unsupervised local learning rules that can still achieve effective learning in multi-layer neural networks (that is, in deep learning).</p>
                    
                    
      -->              
                    <p data-en="Contemporary deep learning is built on three foundational pillars:
Artificial neural networks with at least one hidden layer (that is, at least one layer of neurons between the input and the output),
The formulation of an objective function that takes as input data and learnable parameters (the “synapses”) of the neural network and defines the AI’s performance by outputting a real number; the smaller this number, the better the AI performs, and
The adjustment of the network’s “synapses” using the gradient backpropagation algorithm, which simply means that learning occurs by attempting to minimize the objective function by descending its gradient with respect to the synapses.

However, we know that the human brain does not learn by trying to minimize a global objective function. Instead, it learns through local learning rules at the level of individual neurons, such as Hebbian learning. Despite this, the brain is capable of developing multi-layered deep representations. We also know that local learning, such as Hebbian learning—which does not depend on a global objective function—is more robust, generalizes better beyond the training data (a major shortcoming of current deep learning), and is more resistant to catastrophic forgetting (the inability in current deep learning to learn multiple tasks sequentially without having to learn them all in parallel).
That said, implementing Hebbian learning (or other unsupervised local learning) in artificial neural networks remains very challenging and performs poorly in deep learning contexts. This project addresses this problem and aims to develop unsupervised local learning rules that can still achieve effective learning in multi-layer neural networks."
                       data-fr="L'apprentissage profond contemporain se fonde sur trois piliers porteurs: 1) les réseaux de neurones artificiels avec au moins une couche cachée (c'est-à-dire au moins une couche de neurones entre l'entrée et la sortie), 2) la formulation d'une fonction objective qui prend en entrée des données et des paramètres à apprendre (les « synapses ») du réseau de neurones et qui définie la performance de l'IA en donnant un réel en sortie; plus ce réel est petit mieux est la performance de l'IA, et 3) la modification des « synapses » du réseau de neurones en utilisant l'algorithme de rétropropagation du gradient (backpropagation), qui veut dire simplement que l'apprentissage se fait en essayer de minimizer la fonction objective en descendant le gradient de cette fonction par rapport aux « synapes ». 

Nous savons pourtant que le cerveau humain n'apprend pas, en essayant de minimiser une fonction objective globale, mais plutôt apprend par des règles d'apprentissage locales aux neurones telle l'apprentissage Hebbien. Malgré cela, le cerveau est capables de développer des représentation en plusieurs couches profondes. Nous savons aussi qu'un apprentissage local tel l'apprentissage Hebbien qui n'est pas dépendant d'une fonction objective globale est plus robuste, sait mieux généraliser hors des données d'apprentissage (une grande lacune de l'apprentissage profond actuel), et résiste mieux à l'oublie catastrophique (le fait en apprentissage profond actuel de ne pas être capable d'apprendre plusieurs tâches en série et de devoir absolument les apprendre en parallèle). 

Ceci dit, l'apprentissage Hebbien (ou autre apprentissage local non-supervisé) avec des réseaux neurones artificiels est encore très difficile et peu performant en apprentissage profond. Ce projet s'attaque à ce problème et vise à développer un apprentissage avec des règles locales non-supervisées qui arrivent tout de même à bien apprendre dans un contexte de réseaux neurones multi-couche (c'est-à-dire d'apprentissage profond). ">
                        Contemporary deep learning is built on three foundational pillars:
1) Artificial neural networks with at least one hidden layer (that is, at least one layer of neurons between the input and the output),
2) The formulation of an objective function that takes as input data and learnable parameters (the “synapses”) of the neural network and defines the AI’s performance by outputting a real number; the smaller this number, the better the AI performs, and
3) The adjustment of the network’s “synapses” using the gradient backpropagation algorithm, which simply means that learning occurs by attempting to minimize the objective function by descending its gradient with respect to the synapses.

However, we know that the human brain does not learn by trying to minimize a global objective function. Instead, it learns through local learning rules at the level of individual neurons, such as Hebbian learning. Despite this, the brain is capable of developing multi-layered deep representations. We also know that local learning, such as Hebbian learning—which does not depend on a global objective function—is more robust, generalizes better beyond the training data (a major shortcoming of current deep learning), and is more resistant to catastrophic forgetting (the inability in current deep learning to learn multiple tasks sequentially without having to learn them all in parallel).
That said, implementing Hebbian learning (or other unsupervised local learning) in artificial neural networks remains very challenging and performs poorly in deep learning contexts. This project addresses this problem and aims to develop unsupervised local learning rules that can still achieve effective learning in multi-layer neural networks.
                    </p>
                </div>
            </section>

            <!-- Publications Section - Integrated from Google Scholar -->
            <section class="content-section">
                <h2 data-en="Publications" data-fr="Publications">Publications</h2>
<!--                <p data-en="For a complete list of publications, please visit my Google Scholar profile."
                   data-fr="Pour une liste complète des publications, veuillez consulter mon profil Google Scholar.">
                    For a complete list of publications, please visit my Google Scholar profile.
                </p>-->
                
                <div class="publication-links">
                    <a href="https://www.semanticscholar.org/author/Isabeau-Pr'emont-Schwarz/1403769483" target="_blank" class="btn-primary">
                        <span data-en="View Semantic Scholar Profile" data-fr="Voir le profil Semantic Scholar">View Semantic Scholar Profile</span>
                    </a>
                </div>

                <!-- Optional: Embedded Google Scholar Widget -->
                <!-- You can replace the iframe src with your actual Google Scholar profile -->
                <!--
                <div class="scholar-embed">
                    <iframe src="https://scholar.google.com/citations?user=YOUR_SCHOLAR_ID&hl=en" 
                            width="100%" 
                            height="600" 
                            frameborder="0"></iframe>
                </div>
                -->

                <!-- Alternative: Selected Publications (if you want to manually highlight a few) -->
<!--                <h3 data-en="Selected Recent Publications" data-fr="Publications récentes sélectionnées">Selected Recent Publications</h3>
                <ul class="simple-list">
                    <li data-en="[Leave this section for your most important recent papers if desired, otherwise remove]"
                        data-fr="[Laissez cette section pour vos articles récents les plus importants si désiré, sinon supprimez]">
                        [Leave this section for your most important recent papers if desired, otherwise remove]
                    </li>
                </ul>-->
            </section>

            <!-- Collaborators -->
<!--            <section class="content-section">
                <h2 data-en="Collaborators" data-fr="Collaborateurs">Collaborators</h2>
                <p data-en="I am fortunate to collaborate with researchers from various institutions:"
                   data-fr="J'ai la chance de collaborer avec des chercheurs de diverses institutions :">
                    I am fortunate to collaborate with researchers from various institutions:
                </p>
                <ul class="simple-list">
                    <li><a href="#">[Collaborator Name]</a>, [Institution]</li>
                    <li><a href="#">[Collaborator Name]</a>, [Institution]</li>
                    <li><a href="#">[Collaborator Name]</a>, [Institution]</li>
                </ul>
            </section>-->
        </div>
    </main>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2025 <span data-en="Isabeau Prémont-Schwarz" data-fr="Isabeau Prémont-Schwarz">Isabeau Prémont-Schwarz</span></p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>